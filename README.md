
# 🎯 **Data Scientist Portfolio**  
🌟 Welcome to my **Data Scientist Portfolio** repository! 🚀  
This is a curated collection of my work, showcasing my expertise in **data analysis, machine learning, and visualization**.  
Each project reflects my ability to extract meaningful insights, develop predictive models, and present data-driven solutions effectively.

---

## 📌 **Table of Contents**  
- [🌟 About Me](#-about-me)  
- [🚀 Projects](#-projects)  
- [🛠 Skills](#-skills)  
- [⚙ Tools & Technologies](#-tools--technologies)  
- [💼 Work Experience](#-work-experience)  
- [🎓 Education](#-education)  
- [🎯 Activities](#-activities)  
- [🔧 How to Use This Repository](#-how-to-use-this-repository)  
- [📞 Contact](#-contact)  

---

## 🌟 About Me  
<a id="-about-me"></a>  
I am a **data scientist** with a passion for solving complex problems using data-driven approaches. I specialize in transforming raw data into actionable insights to drive informed decision-making.  
My expertise includes **statistics**, **machine learning**, **data visualization**, and **big data processing**. I am constantly learning and adapting to new technologies to improve my analytical skills and broaden my knowledge in the data science field.

### 📊 **Key Expertise:**  
- **Data Wrangling & Cleaning**  
- **Exploratory Data Analysis (EDA)**  
- **Machine Learning & Predictive Modeling**  
- **Statistical Analysis & Hypothesis Testing**  
- **Data Visualization & Storytelling**  

---


## 🚀 Projects

---

### 🔹 **1. Developed Databases from Scratch using Microsoft SQL Server**  
**Duration:** Jan 2024 - Apr 2024

**Objective:**  
To design and implement a scalable relational database system capable of efficiently managing large volumes of structured data. The system needed to be optimized for performance, with clear data integrity and fast query execution.

**Process:**  
- **Database Design**: Started with analyzing the business requirements to determine the essential entities and their relationships. Created an **Entity-Relationship (ER) Diagram** to visualize the schema.  
- **Normalization**: Applied normalization techniques (up to 3rd Normal Form) to ensure data consistency, reduce redundancy, and improve maintainability.  
- **Table Creation**: Developed tables with primary keys and foreign keys to define relationships between the entities.  
- **Indexing**: Implemented indexing on frequently queried columns to enhance query performance.  
- **Stored Procedures & Triggers**: Wrote stored procedures to automate repetitive tasks like data insertion, updates, and deletions. Triggers were used for enforcing business rules, such as auditing or restricting certain actions.  
- **Performance Optimization**: Used query optimization techniques such as indexing, query refactoring, and minimizing joins to ensure high performance.

**Outcome:**  
- A fully operational relational database system that efficiently manages business data.
- Optimized for high-speed query execution even with large datasets.
- Scalable design that can handle future growth.

**Tools Used:**  
- **Microsoft SQL Server**  
- **T-SQL**

![Database Diagram](/assets/img/Data_Base_Diagram_resized.png)
---

### 🔹 **2. Conducted Statistical Analysis & Advanced Statistics using R**  
**Duration:** Sep 2024 - Dec 2025

**Objective:**  
To conduct in-depth statistical analysis on various datasets to derive insights and support data-driven decision-making. The analysis was aimed at identifying key trends and relationships between variables, helping the stakeholders make informed decisions.

**Process:**  
- **Data Cleaning**: Ensured the dataset was clean by handling missing values, outliers, and inconsistencies. Applied methods like imputation and removal of outliers.  
- **Exploratory Data Analysis (EDA)**: Performed initial analysis to understand the distribution of variables, identify relationships between them, and visualize patterns using histograms, scatter plots, and correlation matrices.  
- **Hypothesis Testing**: Conducted hypothesis tests (e.g., t-tests, chi-square tests) to validate assumptions and draw conclusions.  
- **Regression Analysis**: Built regression models (e.g., linear regression) to quantify relationships between independent and dependent variables.  
- **ANOVA**: Used **Analysis of Variance (ANOVA)** to compare means across different groups and identify significant differences.  
- **Visualization**: Utilized **ggplot2** for creating clear, informative visualizations to convey insights effectively to stakeholders.

**Outcome:**  
- Generated actionable insights that helped stakeholders make data-driven decisions.
- Provided visualizations and reports that communicated findings in an easy-to-understand format.
- Delivered statistical reports that summarized key findings and trends.

**Tools Used:**  
- **R**  
- **ggplot2**, **dplyr**

---

### 🔹 **3. Built Time Series Forecasting Models in R**  
**Duration:** Nov 2024 - Dec 2024

**Objective:**  
To develop accurate time series forecasting models that could predict future values and trends for business operations, helping stakeholders plan and optimize strategies.

**Process:**  
- **Data Preparation**: Collected and prepared historical time series data, ensuring it was stationary (or transformed if necessary).  
- **Model Selection**: Evaluated multiple forecasting models, including **ARIMA (AutoRegressive Integrated Moving Average)**, **SARIMA (Seasonal ARIMA)**, and **Exponential Smoothing State Space Models (ETS)** to determine which one provided the best accuracy.  
- **Model Evaluation**: Used evaluation metrics like **RMSE (Root Mean Squared Error)** and **MAE (Mean Absolute Error)** to measure the performance of each model.  
- **Cross-validation**: Split the data into training and testing sets, performing cross-validation to ensure the model's robustness and generalizability.  
- **Visualization**: Created visualizations comparing the predicted vs actual values to evaluate model performance and visualize forecasted trends.

**Outcome:**  
- Delivered accurate forecasts that helped stakeholders anticipate future trends and make proactive decisions.
- Models were deployed to forecast future demand and optimize inventory management.

**Tools Used:**  
- **R**  
- **forecast**, **tsibble**

---

### 🔹 **4. Designed Power BI Dashboards for Real-Time Insights**  
**Duration:** Jan 2024 - Current

**Objective:**  
To create dynamic and interactive dashboards that allow users to monitor and analyze business performance in real time, helping stakeholders track KPIs and make timely decisions.

**Process:**  
- **Data Integration**: Integrated multiple data sources, including SQL databases, Excel files, and other APIs, into Power BI to ensure data consistency and real-time updates.  
- **Dashboard Design**: Designed user-friendly and visually appealing dashboards that presented key business metrics. Added interactive elements such as slicers and filters to allow users to explore the data from different perspectives.  
- **Advanced Analytics**: Used **DAX (Data Analysis Expressions)** to create calculated measures, columns, and KPIs, enabling more advanced analysis directly within the dashboard.  
- **Performance Optimization**: Ensured that the dashboards were optimized for performance, enabling quick load times even when dealing with large datasets.  
- **Real-Time Data Updates**: Configured Power BI to connect to live data sources, providing real-time insights to the stakeholders.

**Outcome:**  
- Delivered interactive dashboards that allowed stakeholders to monitor performance in real-time.
- Improved decision-making by providing up-to-date, easily interpretable data visualizations.

**Tools Used:**  
- **Power BI**  
- **DAX**, **SQL**

---

### 🔹 **5. Developed Classification & Clustering Models in Python**  
**Duration:** Sep 2024 - Current

**Objective:**  
To build machine learning models for classification and clustering tasks that could predict outcomes and uncover hidden patterns in data.

**Process:**  
- **Data Preprocessing**: Cleaned and preprocessed the data, handling missing values, scaling features, and encoding categorical variables using tools like **Pandas** and **Scikit-learn**.  
- **Model Development**: Built **classification models** such as **Logistic Regression**, **Random Forest**, and **SVM** for binary and multi-class prediction tasks.  
- **Clustering Models**: Implemented **K-Means** and **DBSCAN** for clustering analysis to identify groups of similar data points without predefined labels.  
- **Model Evaluation**: Evaluated classification models using metrics like **accuracy**, **precision**, **recall**, and **F1 score**. Clustering performance was evaluated using the **silhouette score** and other internal validation techniques.  
- **Model Tuning**: Used **GridSearchCV** and **RandomizedSearchCV** to fine-tune model hyperparameters for optimal performance.

**Outcome:**  
- Developed robust machine learning models that provided accurate predictions for various business use cases.
- Identified hidden patterns and clusters within the data, providing valuable insights into customer behavior and other business areas.

**Tools Used:**  
- **Python**  
- **Scikit-learn**, **Pandas**, **Matplotlib**

---

### 🔹 **6. Executed PySpark Projects on Databricks Using RDDs & DataFrames**  
**Duration:** Jan 2024 - Current

**Objective:**  
To process and analyze large-scale datasets efficiently using **PySpark** on the **Databricks** platform. The aim was to handle big data challenges and create scalable, optimized data pipelines.

**Process:**  
- **Data Import**: Used PySpark's **RDDs (Resilient Distributed Datasets)** and **DataFrames** for importing and processing large datasets across distributed systems.  
- **Data Transformation**: Performed data cleaning, aggregation, and transformation tasks using Spark SQL and DataFrame APIs.  
- **Optimization**: Optimized data processing pipelines for performance, reducing processing time significantly by leveraging partitioning, caching, and parallel processing.  
- **Big Data Processing**: Processed datasets too large for traditional systems, providing real-time insights and analytics for business needs.  
- **Visualization**: Visualized processed data using integrated Databricks notebooks for in-depth analysis.

**Outcome:**  
- Created scalable data processing pipelines that allowed the analysis of massive datasets, providing insights in real-time.  
- Improved data analysis efficiency, enabling faster decision-making for data-driven business strategies.

**Tools Used:**  
- **PySpark**  
- **Databricks**, **Apache Spark**

---

## 🛠 Skills  
<a id="-skills"></a>  
Here is a list of my key technical and soft skills.

### **Technical Skills:**  
- **Programming Languages**: Python, R, SQL  
- **Machine Learning**: Classification, Regression, Clustering, Deep Learning  
- **Big Data & Cloud**: Spark, Hadoop, AWS, GCP  
- **Data Visualization**: Power BI, Tableau, Matplotlib, Seaborn  
- **Statistical Analysis**: Hypothesis Testing, ANOVA, Regression Analysis  

### **Soft Skills:**  
- **Problem Solving**  
- **Communication**  
- **Collaboration**  
- **Adaptability**  
- **Critical Thinking**

---

## ⚙ Tools & Technologies  
<a id="-tools--technologies"></a>  
I am proficient in a range of tools and technologies that help me effectively analyze data and develop insights.

- **Programming Languages**: Python, R, SQL  
- **Libraries/Frameworks**: Pandas, NumPy, Scikit-learn, TensorFlow, PyTorch  
- **Databases**: MySQL, PostgreSQL, MongoDB  
- **Big Data & Cloud**: AWS, Azure, GCP, Hadoop, Spark  
- **Data Visualization**: Power BI, Tableau, Matplotlib, Seaborn  
- **Development Tools**: Jupyter Notebook, VS Code, Git

---

## 💼 Work Experience  
<a id="-work-experience"></a>  
I have gained hands-on experience in various data science roles, where I applied my skills to solve real-world business challenges.

### **Data Visualization Analyst (Part-Time)**  
**Eagle Cars & Tiger Taxis | Oct 2024 - Present | Clitheroe, UK**  
- Created weekly and monthly dashboards to report driver performance.  
- Automated reporting processes, reducing manual reporting time by 50%.  
- Developed visualizations that improved decision-making for stakeholders.

### **Data Scientist (Full-Time)**  
**WebDoc | May 2023 - Dec 2023 | Islamabad, Pakistan**  
- Improved data accuracy by 20% through data cleaning and validation.  
- Created over 15 dynamic visualizations to represent complex datasets.  
- Applied regression and classification models to predict user behavior.

### **Data Insights Analyst (Full-Time)**  
**Zones, IT Solutions | Sep 2021 - May 2023 | Islamabad, Pakistan**  
- Developed data-driven strategies that increased customer retention by 18%.  
- Designed and maintained Power BI dashboards for real-time performance tracking.  
- Collaborated with cross-functional teams to design reports for business decisions.

---

## 🎓 Education  
<a id="-education"></a>  
Here’s my academic background that laid the foundation for my career in data science.

### **M.S. in Data Science** _(Expected May 2025)_  
**University of Salford, UK**  
- Coursework: Machine Learning, Big Data Analytics, NLP, Deep Learning

### **B.S. in Software Engineering** _(Graduated May 2022)_  
**Bahria University, Pakistan**  
- Coursework: AI, Data Mining, Web Development, Database Systems

---

## 🎯 Activities  
<a id="-activities"></a>  
In addition to my professional and academic pursuits, I am actively involved in extracurricular activities.

### **President, Dawah Society - Salford University** _(2024)_  
- Organized weekly social events to foster student engagement and unity.

---

## 🔧 How to Use This Repository  
<a id="-how-to-use-this-repository"></a>  
Clone this repository to explore my projects and codebase:  
```bash
git clone https://github.com/your-username/data-scientist-portfolio.git  
cd data-scientist-portfolio

## 📞 Contact
<a id="-contact"></a>
You can get in touch with me through the following channels:

📧 Email: your.email@example.com
🔗 LinkedIn: Your LinkedIn Profile
🐙 GitHub: Your GitHub Profile
